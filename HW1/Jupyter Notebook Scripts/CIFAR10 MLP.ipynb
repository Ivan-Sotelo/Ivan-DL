{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,676,682\n",
      "Trainable params: 3,676,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 2.0146 - acc: 0.2621 - val_loss: 1.8137 - val_acc: 0.3503\n",
      "Epoch 2/200\n",
      " - 2s - loss: 1.8375 - acc: 0.3348 - val_loss: 1.7367 - val_acc: 0.3843\n",
      "Epoch 3/200\n",
      " - 2s - loss: 1.7765 - acc: 0.3620 - val_loss: 1.6987 - val_acc: 0.3980\n",
      "Epoch 4/200\n",
      " - 2s - loss: 1.7368 - acc: 0.3754 - val_loss: 1.6586 - val_acc: 0.4169\n",
      "Epoch 5/200\n",
      " - 2s - loss: 1.7070 - acc: 0.3865 - val_loss: 1.6382 - val_acc: 0.4222\n",
      "Epoch 6/200\n",
      " - 2s - loss: 1.6774 - acc: 0.3982 - val_loss: 1.6114 - val_acc: 0.4295\n",
      "Epoch 7/200\n",
      " - 2s - loss: 1.6558 - acc: 0.4049 - val_loss: 1.6016 - val_acc: 0.4240\n",
      "Epoch 8/200\n",
      " - 2s - loss: 1.6389 - acc: 0.4121 - val_loss: 1.6006 - val_acc: 0.4363\n",
      "Epoch 9/200\n",
      " - 2s - loss: 1.6264 - acc: 0.4136 - val_loss: 1.5908 - val_acc: 0.4353\n",
      "Epoch 10/200\n",
      " - 2s - loss: 1.6082 - acc: 0.4212 - val_loss: 1.5592 - val_acc: 0.4402\n",
      "Epoch 11/200\n",
      " - 2s - loss: 1.5938 - acc: 0.4248 - val_loss: 1.5512 - val_acc: 0.4439\n",
      "Epoch 12/200\n",
      " - 2s - loss: 1.5790 - acc: 0.4294 - val_loss: 1.5432 - val_acc: 0.4511\n",
      "Epoch 13/200\n",
      " - 2s - loss: 1.5686 - acc: 0.4361 - val_loss: 1.5433 - val_acc: 0.4489\n",
      "Epoch 14/200\n",
      " - 2s - loss: 1.5600 - acc: 0.4386 - val_loss: 1.5286 - val_acc: 0.4599\n",
      "Epoch 15/200\n",
      " - 2s - loss: 1.5519 - acc: 0.4448 - val_loss: 1.5185 - val_acc: 0.4620\n",
      "Epoch 16/200\n",
      " - 2s - loss: 1.5441 - acc: 0.4427 - val_loss: 1.5011 - val_acc: 0.4663\n",
      "Epoch 17/200\n",
      " - 2s - loss: 1.5392 - acc: 0.4475 - val_loss: 1.5104 - val_acc: 0.4652\n",
      "Epoch 18/200\n",
      " - 2s - loss: 1.5318 - acc: 0.4486 - val_loss: 1.5132 - val_acc: 0.4655\n",
      "Epoch 19/200\n",
      " - 2s - loss: 1.5240 - acc: 0.4497 - val_loss: 1.4979 - val_acc: 0.4677\n",
      "Epoch 20/200\n",
      " - 2s - loss: 1.5149 - acc: 0.4563 - val_loss: 1.4736 - val_acc: 0.4775\n",
      "Epoch 21/200\n",
      " - 2s - loss: 1.5053 - acc: 0.4565 - val_loss: 1.4967 - val_acc: 0.4751\n",
      "Epoch 22/200\n",
      " - 2s - loss: 1.5072 - acc: 0.4596 - val_loss: 1.4996 - val_acc: 0.4630\n",
      "Epoch 23/200\n",
      " - 2s - loss: 1.4943 - acc: 0.4608 - val_loss: 1.4709 - val_acc: 0.4808\n",
      "Epoch 24/200\n",
      " - 2s - loss: 1.4883 - acc: 0.4665 - val_loss: 1.4852 - val_acc: 0.4790\n",
      "Epoch 25/200\n",
      " - 2s - loss: 1.4843 - acc: 0.4653 - val_loss: 1.4680 - val_acc: 0.4826\n",
      "Epoch 26/200\n",
      " - 2s - loss: 1.4826 - acc: 0.4660 - val_loss: 1.4820 - val_acc: 0.4787\n",
      "Epoch 27/200\n",
      " - 2s - loss: 1.4763 - acc: 0.4657 - val_loss: 1.4648 - val_acc: 0.4833\n",
      "Epoch 28/200\n",
      " - 2s - loss: 1.4805 - acc: 0.4660 - val_loss: 1.4657 - val_acc: 0.4834\n",
      "Epoch 29/200\n",
      " - 2s - loss: 1.4601 - acc: 0.4748 - val_loss: 1.4538 - val_acc: 0.4844\n",
      "Epoch 30/200\n",
      " - 2s - loss: 1.4481 - acc: 0.4798 - val_loss: 1.4515 - val_acc: 0.4928\n",
      "Epoch 31/200\n",
      " - 2s - loss: 1.4587 - acc: 0.4728 - val_loss: 1.4515 - val_acc: 0.4835\n",
      "Epoch 32/200\n",
      " - 2s - loss: 1.4458 - acc: 0.4800 - val_loss: 1.4406 - val_acc: 0.4953\n",
      "Epoch 33/200\n",
      " - 2s - loss: 1.4451 - acc: 0.4799 - val_loss: 1.4464 - val_acc: 0.4823\n",
      "Epoch 34/200\n",
      " - 2s - loss: 1.4459 - acc: 0.4812 - val_loss: 1.4437 - val_acc: 0.4899\n",
      "Epoch 35/200\n",
      " - 2s - loss: 1.4392 - acc: 0.4822 - val_loss: 1.4456 - val_acc: 0.4927\n",
      "Epoch 36/200\n",
      " - 2s - loss: 1.4384 - acc: 0.4818 - val_loss: 1.4424 - val_acc: 0.4901\n",
      "Epoch 37/200\n",
      " - 2s - loss: 1.4255 - acc: 0.4839 - val_loss: 1.4316 - val_acc: 0.4959\n",
      "Epoch 38/200\n",
      " - 2s - loss: 1.4257 - acc: 0.4831 - val_loss: 1.4375 - val_acc: 0.4927\n",
      "Epoch 39/200\n",
      " - 2s - loss: 1.4219 - acc: 0.4887 - val_loss: 1.4433 - val_acc: 0.4925\n",
      "Epoch 40/200\n",
      " - 2s - loss: 1.4169 - acc: 0.4903 - val_loss: 1.4571 - val_acc: 0.4871\n",
      "Epoch 41/200\n",
      " - 2s - loss: 1.4146 - acc: 0.4902 - val_loss: 1.4323 - val_acc: 0.4977\n",
      "Epoch 42/200\n",
      " - 2s - loss: 1.4111 - acc: 0.4919 - val_loss: 1.4119 - val_acc: 0.4987\n",
      "Epoch 43/200\n",
      " - 2s - loss: 1.4143 - acc: 0.4919 - val_loss: 1.4265 - val_acc: 0.4933\n",
      "Epoch 44/200\n",
      " - 2s - loss: 1.4105 - acc: 0.4929 - val_loss: 1.4637 - val_acc: 0.4830\n",
      "Epoch 45/200\n",
      " - 2s - loss: 1.4049 - acc: 0.4909 - val_loss: 1.4273 - val_acc: 0.4943\n",
      "Epoch 46/200\n",
      " - 2s - loss: 1.4032 - acc: 0.4952 - val_loss: 1.4302 - val_acc: 0.4947\n",
      "Epoch 47/200\n",
      " - 2s - loss: 1.4046 - acc: 0.4951 - val_loss: 1.4369 - val_acc: 0.4969\n",
      "Epoch 48/200\n",
      " - 2s - loss: 1.3924 - acc: 0.4995 - val_loss: 1.4312 - val_acc: 0.4960\n",
      "Epoch 49/200\n",
      " - 2s - loss: 1.3887 - acc: 0.4982 - val_loss: 1.4224 - val_acc: 0.4965\n",
      "Epoch 50/200\n",
      " - 2s - loss: 1.3978 - acc: 0.4959 - val_loss: 1.4156 - val_acc: 0.4991\n",
      "Epoch 51/200\n",
      " - 2s - loss: 1.3828 - acc: 0.5009 - val_loss: 1.4095 - val_acc: 0.5031\n",
      "Epoch 52/200\n",
      " - 2s - loss: 1.3895 - acc: 0.5005 - val_loss: 1.4176 - val_acc: 0.5012\n",
      "Epoch 53/200\n",
      " - 2s - loss: 1.3850 - acc: 0.5003 - val_loss: 1.4265 - val_acc: 0.5050\n",
      "Epoch 54/200\n",
      " - 2s - loss: 1.3758 - acc: 0.5063 - val_loss: 1.4269 - val_acc: 0.4979\n",
      "Epoch 55/200\n",
      " - 2s - loss: 1.3754 - acc: 0.5056 - val_loss: 1.4190 - val_acc: 0.4990\n",
      "Epoch 56/200\n",
      " - 2s - loss: 1.3812 - acc: 0.5042 - val_loss: 1.4339 - val_acc: 0.5027\n",
      "Epoch 57/200\n",
      " - 2s - loss: 1.3648 - acc: 0.5091 - val_loss: 1.4134 - val_acc: 0.5012\n",
      "Epoch 58/200\n",
      " - 2s - loss: 1.3580 - acc: 0.5109 - val_loss: 1.4304 - val_acc: 0.4917\n",
      "Epoch 59/200\n",
      " - 2s - loss: 1.3599 - acc: 0.5125 - val_loss: 1.4139 - val_acc: 0.5002\n",
      "Epoch 60/200\n",
      " - 2s - loss: 1.3632 - acc: 0.5076 - val_loss: 1.4215 - val_acc: 0.4990\n",
      "Epoch 61/200\n",
      " - 2s - loss: 1.3589 - acc: 0.5109 - val_loss: 1.4075 - val_acc: 0.5075\n",
      "Epoch 62/200\n",
      " - 2s - loss: 1.3636 - acc: 0.5099 - val_loss: 1.4160 - val_acc: 0.5039\n",
      "Epoch 63/200\n",
      " - 2s - loss: 1.3506 - acc: 0.5176 - val_loss: 1.4071 - val_acc: 0.4976\n",
      "Epoch 64/200\n",
      " - 2s - loss: 1.3508 - acc: 0.5131 - val_loss: 1.4364 - val_acc: 0.4988\n",
      "Epoch 65/200\n",
      " - 2s - loss: 1.3463 - acc: 0.5129 - val_loss: 1.4170 - val_acc: 0.4975\n",
      "Epoch 66/200\n",
      " - 2s - loss: 1.3513 - acc: 0.5119 - val_loss: 1.4053 - val_acc: 0.5058\n",
      "Epoch 67/200\n",
      " - 2s - loss: 1.3541 - acc: 0.5136 - val_loss: 1.4085 - val_acc: 0.5078\n",
      "Epoch 68/200\n",
      " - 2s - loss: 1.3487 - acc: 0.5171 - val_loss: 1.4104 - val_acc: 0.5003\n",
      "Epoch 69/200\n",
      " - 2s - loss: 1.3551 - acc: 0.5124 - val_loss: 1.4334 - val_acc: 0.4974\n",
      "Epoch 70/200\n",
      " - 2s - loss: 1.3429 - acc: 0.5172 - val_loss: 1.4148 - val_acc: 0.5019\n",
      "Epoch 71/200\n",
      " - 2s - loss: 1.3459 - acc: 0.5144 - val_loss: 1.4057 - val_acc: 0.5074\n",
      "Epoch 72/200\n",
      " - 2s - loss: 1.3335 - acc: 0.5205 - val_loss: 1.4105 - val_acc: 0.4985\n",
      "Epoch 73/200\n",
      " - 2s - loss: 1.3408 - acc: 0.5162 - val_loss: 1.4091 - val_acc: 0.5042\n",
      "Epoch 74/200\n",
      " - 2s - loss: 1.3274 - acc: 0.5216 - val_loss: 1.4001 - val_acc: 0.5101\n",
      "Epoch 75/200\n",
      " - 2s - loss: 1.3387 - acc: 0.5191 - val_loss: 1.4037 - val_acc: 0.5039\n",
      "Epoch 76/200\n",
      " - 2s - loss: 1.3269 - acc: 0.5197 - val_loss: 1.4172 - val_acc: 0.5015\n",
      "Epoch 77/200\n",
      " - 2s - loss: 1.3244 - acc: 0.5227 - val_loss: 1.4067 - val_acc: 0.5087\n",
      "Epoch 78/200\n",
      " - 2s - loss: 1.3236 - acc: 0.5204 - val_loss: 1.4045 - val_acc: 0.5058\n",
      "Epoch 79/200\n",
      " - 2s - loss: 1.3234 - acc: 0.5233 - val_loss: 1.4085 - val_acc: 0.5042\n",
      "Epoch 80/200\n",
      " - 2s - loss: 1.3248 - acc: 0.5215 - val_loss: 1.4059 - val_acc: 0.5073\n",
      "Epoch 81/200\n",
      " - 2s - loss: 1.3231 - acc: 0.5216 - val_loss: 1.4284 - val_acc: 0.4970\n",
      "Epoch 82/200\n",
      " - 2s - loss: 1.3143 - acc: 0.5255 - val_loss: 1.4090 - val_acc: 0.5071\n",
      "Epoch 83/200\n",
      " - 2s - loss: 1.3183 - acc: 0.5239 - val_loss: 1.4237 - val_acc: 0.5014\n",
      "Epoch 84/200\n",
      " - 2s - loss: 1.3130 - acc: 0.5282 - val_loss: 1.3953 - val_acc: 0.5085\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 1.3134 - acc: 0.5246 - val_loss: 1.4026 - val_acc: 0.5061\n",
      "Epoch 86/200\n",
      " - 2s - loss: 1.3140 - acc: 0.5266 - val_loss: 1.4142 - val_acc: 0.5012\n",
      "Epoch 87/200\n",
      " - 2s - loss: 1.3044 - acc: 0.5328 - val_loss: 1.3934 - val_acc: 0.5133\n",
      "Epoch 88/200\n",
      " - 2s - loss: 1.3082 - acc: 0.5292 - val_loss: 1.4205 - val_acc: 0.4987\n",
      "Epoch 89/200\n",
      " - 2s - loss: 1.3114 - acc: 0.5276 - val_loss: 1.3982 - val_acc: 0.5105\n",
      "Epoch 90/200\n",
      " - 2s - loss: 1.3018 - acc: 0.5328 - val_loss: 1.4292 - val_acc: 0.4995\n",
      "Epoch 91/200\n",
      " - 2s - loss: 1.2994 - acc: 0.5301 - val_loss: 1.4152 - val_acc: 0.4969\n",
      "Epoch 92/200\n",
      " - 2s - loss: 1.2986 - acc: 0.5303 - val_loss: 1.3976 - val_acc: 0.5104\n",
      "Epoch 93/200\n",
      " - 2s - loss: 1.2992 - acc: 0.5319 - val_loss: 1.4030 - val_acc: 0.5140\n",
      "Epoch 94/200\n",
      " - 2s - loss: 1.2915 - acc: 0.5358 - val_loss: 1.4246 - val_acc: 0.5002\n",
      "Epoch 95/200\n",
      " - 2s - loss: 1.2977 - acc: 0.5321 - val_loss: 1.3966 - val_acc: 0.5097\n",
      "Epoch 96/200\n",
      " - 2s - loss: 1.2949 - acc: 0.5348 - val_loss: 1.4052 - val_acc: 0.5117\n",
      "Epoch 97/200\n",
      " - 2s - loss: 1.2807 - acc: 0.5381 - val_loss: 1.4011 - val_acc: 0.5113\n",
      "Epoch 98/200\n",
      " - 2s - loss: 1.2981 - acc: 0.5323 - val_loss: 1.4003 - val_acc: 0.5109\n",
      "Epoch 99/200\n",
      " - 2s - loss: 1.2858 - acc: 0.5371 - val_loss: 1.4047 - val_acc: 0.5046\n",
      "Epoch 100/200\n",
      " - 2s - loss: 1.2853 - acc: 0.5389 - val_loss: 1.4059 - val_acc: 0.5041\n",
      "Epoch 101/200\n",
      " - 2s - loss: 1.2857 - acc: 0.5356 - val_loss: 1.4074 - val_acc: 0.5047\n",
      "Epoch 102/200\n",
      " - 2s - loss: 1.2761 - acc: 0.5403 - val_loss: 1.4016 - val_acc: 0.5070\n",
      "Epoch 103/200\n",
      " - 2s - loss: 1.2782 - acc: 0.5362 - val_loss: 1.4001 - val_acc: 0.5127\n",
      "Epoch 104/200\n",
      " - 2s - loss: 1.2753 - acc: 0.5410 - val_loss: 1.3971 - val_acc: 0.5117\n",
      "Epoch 105/200\n",
      " - 2s - loss: 1.2808 - acc: 0.5381 - val_loss: 1.4358 - val_acc: 0.4955\n",
      "Epoch 106/200\n",
      " - 2s - loss: 1.2811 - acc: 0.5377 - val_loss: 1.4084 - val_acc: 0.5043\n",
      "Epoch 107/200\n",
      " - 2s - loss: 1.2706 - acc: 0.5412 - val_loss: 1.4180 - val_acc: 0.5034\n",
      "Epoch 108/200\n",
      " - 2s - loss: 1.2745 - acc: 0.5395 - val_loss: 1.4227 - val_acc: 0.4944\n",
      "Epoch 109/200\n",
      " - 2s - loss: 1.2711 - acc: 0.5414 - val_loss: 1.4116 - val_acc: 0.5099\n",
      "Epoch 110/200\n",
      " - 2s - loss: 1.2757 - acc: 0.5404 - val_loss: 1.4118 - val_acc: 0.5034\n",
      "Epoch 111/200\n",
      " - 2s - loss: 1.2622 - acc: 0.5428 - val_loss: 1.4045 - val_acc: 0.5098\n",
      "Epoch 112/200\n",
      " - 2s - loss: 1.2697 - acc: 0.5422 - val_loss: 1.4070 - val_acc: 0.5083\n",
      "Epoch 113/200\n",
      " - 2s - loss: 1.2606 - acc: 0.5434 - val_loss: 1.4032 - val_acc: 0.5063\n",
      "Epoch 114/200\n",
      " - 2s - loss: 1.2550 - acc: 0.5466 - val_loss: 1.4052 - val_acc: 0.5022\n",
      "Epoch 115/200\n",
      " - 2s - loss: 1.2636 - acc: 0.5454 - val_loss: 1.4036 - val_acc: 0.5090\n",
      "Epoch 116/200\n",
      " - 2s - loss: 1.2702 - acc: 0.5444 - val_loss: 1.4077 - val_acc: 0.5097\n",
      "Epoch 117/200\n",
      " - 2s - loss: 1.2642 - acc: 0.5443 - val_loss: 1.3936 - val_acc: 0.5110\n",
      "Epoch 118/200\n",
      " - 2s - loss: 1.2585 - acc: 0.5465 - val_loss: 1.4412 - val_acc: 0.5020\n",
      "Epoch 119/200\n",
      " - 2s - loss: 1.2615 - acc: 0.5484 - val_loss: 1.4027 - val_acc: 0.5127\n",
      "Epoch 120/200\n",
      " - 2s - loss: 1.2611 - acc: 0.5443 - val_loss: 1.4141 - val_acc: 0.5018\n",
      "Epoch 121/200\n",
      " - 2s - loss: 1.2605 - acc: 0.5437 - val_loss: 1.4197 - val_acc: 0.5057\n",
      "Epoch 122/200\n",
      " - 2s - loss: 1.2494 - acc: 0.5501 - val_loss: 1.4065 - val_acc: 0.5103\n",
      "Epoch 123/200\n",
      " - 2s - loss: 1.2465 - acc: 0.5500 - val_loss: 1.4134 - val_acc: 0.5061\n",
      "Epoch 124/200\n",
      " - 2s - loss: 1.2531 - acc: 0.5489 - val_loss: 1.4216 - val_acc: 0.5011\n",
      "Epoch 125/200\n",
      " - 2s - loss: 1.2609 - acc: 0.5442 - val_loss: 1.4200 - val_acc: 0.5045\n",
      "Epoch 126/200\n",
      " - 2s - loss: 1.2429 - acc: 0.5503 - val_loss: 1.4117 - val_acc: 0.5123\n",
      "Epoch 127/200\n",
      " - 2s - loss: 1.2503 - acc: 0.5472 - val_loss: 1.4257 - val_acc: 0.4994\n",
      "Epoch 128/200\n",
      " - 2s - loss: 1.2488 - acc: 0.5490 - val_loss: 1.3991 - val_acc: 0.5175\n",
      "Epoch 129/200\n",
      " - 2s - loss: 1.2421 - acc: 0.5539 - val_loss: 1.4073 - val_acc: 0.5091\n",
      "Epoch 130/200\n",
      " - 2s - loss: 1.2503 - acc: 0.5484 - val_loss: 1.4125 - val_acc: 0.5070\n",
      "Epoch 131/200\n",
      " - 2s - loss: 1.2441 - acc: 0.5524 - val_loss: 1.4048 - val_acc: 0.5042\n",
      "Epoch 132/200\n",
      " - 2s - loss: 1.2331 - acc: 0.5555 - val_loss: 1.4077 - val_acc: 0.5066\n",
      "Epoch 133/200\n",
      " - 2s - loss: 1.2319 - acc: 0.5542 - val_loss: 1.4158 - val_acc: 0.5045\n",
      "Epoch 134/200\n",
      " - 2s - loss: 1.2345 - acc: 0.5548 - val_loss: 1.4087 - val_acc: 0.5076\n",
      "Epoch 135/200\n",
      " - 2s - loss: 1.2429 - acc: 0.5496 - val_loss: 1.4028 - val_acc: 0.5072\n",
      "Epoch 136/200\n",
      " - 2s - loss: 1.2396 - acc: 0.5516 - val_loss: 1.3939 - val_acc: 0.5135\n",
      "Epoch 137/200\n",
      " - 2s - loss: 1.2281 - acc: 0.5555 - val_loss: 1.4210 - val_acc: 0.5088\n",
      "Epoch 138/200\n",
      " - 2s - loss: 1.2362 - acc: 0.5554 - val_loss: 1.4124 - val_acc: 0.5060\n",
      "Epoch 139/200\n",
      " - 2s - loss: 1.2342 - acc: 0.5548 - val_loss: 1.4336 - val_acc: 0.5056\n",
      "Epoch 140/200\n",
      " - 2s - loss: 1.2414 - acc: 0.5513 - val_loss: 1.4119 - val_acc: 0.5134\n",
      "Epoch 141/200\n",
      " - 2s - loss: 1.2259 - acc: 0.5590 - val_loss: 1.4130 - val_acc: 0.5128\n",
      "Epoch 142/200\n",
      " - 2s - loss: 1.2327 - acc: 0.5543 - val_loss: 1.4236 - val_acc: 0.5005\n",
      "Epoch 143/200\n",
      " - 2s - loss: 1.2278 - acc: 0.5595 - val_loss: 1.4281 - val_acc: 0.5033\n",
      "Epoch 144/200\n",
      " - 2s - loss: 1.2273 - acc: 0.5574 - val_loss: 1.4266 - val_acc: 0.5072\n",
      "Epoch 145/200\n",
      " - 2s - loss: 1.2292 - acc: 0.5584 - val_loss: 1.4112 - val_acc: 0.5044\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Loading the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#transform labesls to one hot vector\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "#Image to Vector \n",
    "X_train = np.reshape(X_train,(50000,3072))\n",
    "X_test = np.reshape(X_test,(10000,3072))\n",
    "\n",
    "#normalization [0,1]\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "\n",
    "#3 layer MLP \n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=3072))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=0.0006, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "\n",
    "#compiler parameters\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "              \n",
    "#training Parameters\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "history = model.fit(x=X_train,y=y_train,batch_size = batch_size,epochs=epochs, verbose=2,validation_split=0.2)\n",
    "\n",
    "#plotting of train error and tst error\n",
    "def plotLosses(history):  \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plotLosses(history)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))             \n",
    "              \n",
    "\n",
    "              \n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
