{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "#tensorflow2 libs\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 64, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declarations of dataset specs\n",
    "train_data_dir = 'data/train'\n",
    "test_data_dir = 'data/test'\n",
    "nb_train_samples = 4672\n",
    "nb_validation_samples = 660\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "num_classes = 7\n",
    "top_model_weights_path = 'best_model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " #adding zero padding in images\n",
    "base_model = Sequential()                                                                                                                                                                              \n",
    "base_model.add(ZeroPadding2D((80, 80), input_shape=(64, 64, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Resnet50\n",
    "new_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "# base_model = Sequential()\n",
    "base_model.add(tf.keras.applications.ResNet50(input_tensor=new_input, weights='imagenet', include_top=False))\n",
    "print('Model loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 7)                 25692167  \n",
      "=================================================================\n",
      "Total params: 49,279,879\n",
      "Trainable params: 25,692,167\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#freezing layers for regularization\n",
    "for i in range(len(base_model.layers)):\n",
    "    base_model.layers[i].trainable = False\n",
    "\n",
    "\n",
    "# add new classifier layers\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "#add top model to base\n",
    "base_model.add(top_model)\n",
    "\n",
    "#load weights of top model\n",
    "# base_model.load_weights(top_model_weights_path)\n",
    "print(\"Created model and loaded weights from file\")\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "base_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4672 images belonging to 7 classes.\n",
      "Found 660 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#inputting the dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"best_boi.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 4s 178ms/step - loss: 0.9081 - acc: 0.3245\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.32446, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 30s 206ms/step - loss: 0.5290 - acc: 0.8043 - val_loss: 0.9081 - val_acc: 0.3245\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.8029 - acc: 0.5853\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.32446 to 0.58528, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 25s 174ms/step - loss: 0.4340 - acc: 0.8349 - val_loss: 0.8029 - val_acc: 0.5853\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.7046 - acc: 0.6232\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58528 to 0.62316, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 25s 174ms/step - loss: 0.4175 - acc: 0.8420 - val_loss: 0.7046 - val_acc: 0.6232\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.5443 - acc: 0.7424\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62316 to 0.74242, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 25s 174ms/step - loss: 0.4109 - acc: 0.8450 - val_loss: 0.5443 - val_acc: 0.7424\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.4216 - acc: 0.8435\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74242 to 0.84351, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 25s 173ms/step - loss: 0.4053 - acc: 0.8447 - val_loss: 0.4216 - val_acc: 0.8435\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.3847 - acc: 0.8554\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.84351 to 0.85541, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 26s 180ms/step - loss: 0.3972 - acc: 0.8496 - val_loss: 0.3847 - val_acc: 0.8554\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.3845 - acc: 0.8539\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85541\n",
      "146/146 [==============================] - 24s 167ms/step - loss: 0.3967 - acc: 0.8501 - val_loss: 0.3845 - val_acc: 0.8539\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.3762 - acc: 0.8552\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85541\n",
      "146/146 [==============================] - 24s 167ms/step - loss: 0.3906 - acc: 0.8500 - val_loss: 0.3762 - val_acc: 0.8552\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.3758 - acc: 0.8541\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85541\n",
      "146/146 [==============================] - 24s 167ms/step - loss: 0.3856 - acc: 0.8519 - val_loss: 0.3758 - val_acc: 0.8541\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 3s 131ms/step - loss: 0.3713 - acc: 0.8597\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85541 to 0.85974, saving model to best_boi.hdf5\n",
      "146/146 [==============================] - 25s 173ms/step - loss: 0.3853 - acc: 0.8523 - val_loss: 0.3713 - val_acc: 0.8597\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#running the model\n",
    "history = base_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting data into variables\n",
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = validation_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/sample - loss: 0.3783 - acc: 0.8705\n",
      "Test loss: 0.3783434331417084\n",
      "Test accuracy: 0.87053573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#getting evaluation scores\n",
    "score = base_model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n",
    "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
    "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
    "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving weights\n",
    "# base_model.save_weights('fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
